{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 1: Introduction to LangGraph & State Management\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand what LangGraph is and why we need it\n",
    "- Learn core concepts: nodes, edges, state, checkpointers\n",
    "- Build a simple stateful chatbot with conversation memory\n",
    "- Compare LangGraph with LangChain chains\n",
    "\n",
    "**Prerequisites:** RAG with LangChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: What is LangGraph?\n",
    "\n",
    "### The Problem with Chains\n",
    "\n",
    "You have learned about **LangChain chains** - they're great for fixed pipelines:\n",
    "\n",
    "```python\n",
    "# Always follows this path:\n",
    "chain = retriever | prompt | llm | parser\n",
    "```\n",
    "\n",
    "But what if you need:\n",
    "- **Decisions during execution?** (\"Should I retrieve or not?\")\n",
    "- **Loops and cycles?** (\"Try again if answer is poor\")\n",
    "- **Multiple tools?** (\"Use search OR calculator OR retrieval\")\n",
    "- **Complex control flow?** (\"If X then Y, else Z\")\n",
    "\n",
    "**That's where LangGraph comes in!**\n",
    "\n",
    "### LangGraph = State Machines for Agents\n",
    "\n",
    "LangGraph lets you build **graphs** where:\n",
    "- **Nodes** are functions that process state\n",
    "- **Edges** connect nodes (fixed or conditional)\n",
    "- **State** flows through the graph\n",
    "- **Agents make decisions** about which path to take\n",
    "\n",
    "```\n",
    "           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  START ‚îÄ‚îÄ‚ñ∂‚îÇ  Node 1  ‚îÇ\n",
    "           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                ‚îÇ\n",
    "                ‚ñº\n",
    "           Decision?\n",
    "           ‚îú‚îÄ YES ‚îÄ‚ñ∂ Node 2 ‚îÄ‚îÄ‚îê\n",
    "           ‚îî‚îÄ NO  ‚îÄ‚ñ∂ Node 3 ‚îÄ‚îÄ‚î§\n",
    "                               ‚ñº\n",
    "                              END\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LangGraph Application Architecture](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/LangGraph/application.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Setup\n",
    "\n",
    "First, let's install the required packages and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install -q langgraph langchain langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up OpenAI API Key\n",
    "\n",
    "Create a `.env` file in your project directory with:\n",
    "```\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found! Please set it in your .env file.\")\n",
    "\n",
    "print(\"‚úÖ API key loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the LLM\n",
    "\n",
    "We'll use **GPT-4o-mini** - it's fast and cost-effective for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialized: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    api_key=openai_api_key\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ LLM initialized: {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Core Concept #1 - State\n",
    "\n",
    "**State** ‚ÄúThe single source of truth for the whole agent execution.‚Äù  \n",
    "**State** is the data that flows through your graph.\n",
    "\n",
    "### MessagesState\n",
    "\n",
    "For chatbots, LangGraph provides `MessagesState` - it stores conversation history:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"Hello\"),\n",
    "        AIMessage(content=\"Hi! How can I help?\"),\n",
    "        HumanMessage(content=\"What's Python?\"),\n",
    "        # ... more messages\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "This is similar to `ConversationBufferMemory`, but more flexible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ü§î Reflection Question:** \n",
    "How is this different from ConversationBufferMemory? In chains, memory was managed separately. In LangGraph, it's part of the state that flows through nodes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Core Concept #2 - Nodes\n",
    "\n",
    "**Nodes** are functions that process state and return updates.\n",
    "\n",
    "### Node Function Signature\n",
    "\n",
    "```python\n",
    "def my_node(state: MessagesState) -> dict:\n",
    "    # Process state\n",
    "    # Return updates to state\n",
    "    return {\"messages\": [new_message]}\n",
    "```\n",
    "\n",
    "### The Assistant Node\n",
    "\n",
    "Let's create our first node - it sends messages to the LLM and gets a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Assistant node defined\n"
     ]
    }
   ],
   "source": [
    "# System prompt that defines assistant behavior\n",
    "sys_msg = SystemMessage(\n",
    "    content=\"You are a friendly assistant that answers user questions. Be helpful and concise.\"\n",
    ")\n",
    "\n",
    "def assistant(state: MessagesState) -> dict:\n",
    "    \"\"\"\n",
    "    The assistant node - processes messages and generates response.\n",
    "    \"\"\"\n",
    "    # Combine system prompt with conversation history\n",
    "    messages = [sys_msg] + state[\"messages\"]\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Return as state update\n",
    "    return {\"messages\": [AIMessage(content=response.content)]}\n",
    "\n",
    "print(\"‚úÖ Assistant node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Key Point:** The node doesn't modify state directly - it returns updates that LangGraph applies automatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of a Retriever Node\n",
    "You don't need to run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retrieve_docs(state: MessagesState):\n",
    "#     query = state[\"messages\"][-1].content  # latest HumanMessage\n",
    "#     docs = retriever.invoke(query)\n",
    "\n",
    "#     return {\n",
    "#         \"messages\": [\n",
    "#             ToolMessage(\n",
    "#                 content=\"\\n\".join(d.page_content for d in docs),\n",
    "#                 name=\"retriever\"\n",
    "#             )\n",
    "#         ]\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Core Concept 2B - Edges\n",
    "\n",
    "**Edges** are the connections between nodes that control the flow of your agent.\n",
    "\n",
    "Think of edges as **roads** between cities (nodes). They determine which node to visit next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Types of Edges\n",
    "\n",
    "LangGraph has two types of edges:\n",
    "\n",
    "1. **Fixed/Static Edges** (Normal Edges)\n",
    "   - Always go from Node A to Node B\n",
    "   - No decision-making\n",
    "   - Used for predictable flows\n",
    "\n",
    "2. **Conditional Edges**\n",
    "   - Decide which node to visit next based on state\n",
    "   - Enable agent decision-making\n",
    "   - Used for dynamic, intelligent behavior\n",
    "\n",
    "```\n",
    "Fixed/Static Edge:\n",
    "  Node A ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Node B  (always goes to B)\n",
    "\n",
    "Conditional Edge:\n",
    "  Node A ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Decision?\n",
    "                    ‚îú‚îÄ Condition 1 ‚îÄ‚îÄ‚ñ∂ Node B\n",
    "                    ‚îú‚îÄ Condition 2 ‚îÄ‚îÄ‚ñ∂ Node C\n",
    "                    ‚îî‚îÄ Condition 3 ‚îÄ‚îÄ‚ñ∂ Node D\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Core Concept #3 - Building the Graph\n",
    "\n",
    "Now let's connect everything into a **StateGraph**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Graph structure defined\n"
     ]
    }
   ],
   "source": [
    "# Create a StateGraph with MessagesState\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add the assistant node\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "\n",
    "# Define the flow:\n",
    "# START ‚Üí assistant ‚Üí END\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_edge(\"assistant\", END)\n",
    "\n",
    "print(\"‚úÖ Graph structure defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Flow\n",
    "\n",
    "```\n",
    "START ‚Üí [assistant node] ‚Üí END\n",
    "```\n",
    "\n",
    "- **START:** Entry point (receives user message)\n",
    "- **assistant:** Processes message and generates response\n",
    "- **END:** Exit point (returns final state)\n",
    "\n",
    "This is simple now, but later we'll add conditional edges for agentic behavior!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Core Concept #4 - Checkpointers (Memory)\n",
    "\n",
    "**Checkpointers** save state between interactions - this gives our agent memory!\n",
    "\n",
    "Without checkpointer:\n",
    "- Each call starts fresh\n",
    "- No conversation history\n",
    "- Agent forgets everything\n",
    "\n",
    "With checkpointer:\n",
    "- State persists between calls\n",
    "- Agent remembers conversation\n",
    "- Multi-turn conversations work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent compiled with memory\n"
     ]
    }
   ],
   "source": [
    "# Create a memory checkpointer (stores in memory)\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile the graph WITH memory\n",
    "agent = builder.compile(checkpointer=memory)\n",
    "\n",
    "print(\"‚úÖ Agent compiled with memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîß Production Note:** `MemorySaver` stores in RAM (lost on restart). For production, use:\n",
    "- `SqliteSaver` - persists to SQLite database\n",
    "- `MongoDBSaver` - persists to MongoDB\n",
    "\n",
    "We'll use MemorySaver for learning since it's simple!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Visualizing the Graph\n",
    "\n",
    "One of LangGraph's best features - **visual representation** of your agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the graph structure\n",
    "try:\n",
    "    display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")\n",
    "    print(\"Graph structure: START ‚Üí assistant ‚Üí END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Pro Tip:** Always visualize your graph! It helps you understand and debug agent behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Running the Agent\n",
    "\n",
    "Now let's actually use our agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session IDs (Thread IDs)\n",
    "\n",
    "Each conversation has a unique **thread_id**. Messages with the same thread_id share memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation with session ID: chat-session-0012\n"
     ]
    }
   ],
   "source": [
    "# Define a session ID for this conversation\n",
    "session_id = \"chat-session-0012\"\n",
    "\n",
    "print(f\"Starting conversation with session ID: {session_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function for Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conversation function ready\n"
     ]
    }
   ],
   "source": [
    "def run_conversation(user_input: str, thread_id: str = session_id):\n",
    "    \"\"\"\n",
    "    Send a message to the agent and get response.\n",
    "    ‚ö†Ô∏è WARNING: Using default thread_id shares conversation acrosss all calls!\n",
    "    In production, ALWAYS provide unique thread_id per user.\n",
    "    \"\"\"\n",
    "    # Invoke the agent\n",
    "    result = agent.invoke(\n",
    "        {\"messages\": [HumanMessage(content=user_input)]},\n",
    "        config={\"configurable\": {\"thread_id\": thread_id}}\n",
    "    )\n",
    "    \n",
    "    # Print the conversation\n",
    "    for message in result[\"messages\"]:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"\\nüë§ User: {message.content}\")\n",
    "        elif isinstance(message, AIMessage):\n",
    "            print(f\"ü§ñ Agent: {message.content}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(\"‚úÖ Conversation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Single Turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: Hello! What's your name?\n",
      "ü§ñ Agent: Hello! I don‚Äôt have a personal name, but you can call me Assistant. How can I help you today?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "run_conversation(\"Hello! What's your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Multi-Turn Conversation (Memory Test!)\n",
    "\n",
    "Now let's test if the agent remembers context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: Hello! What's your name?\n",
      "ü§ñ Agent: Hello! I don‚Äôt have a personal name, but you can call me Assistant. How can I help you today?\n",
      "\n",
      "üë§ User: My favorite color is blue\n",
      "ü§ñ Agent: That's great! Blue is often associated with calmness and tranquility. Do you have a favorite shade of blue?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# First message\n",
    "run_conversation(\"My favorite color is blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: Hello! What's your name?\n",
      "ü§ñ Agent: Hello! I don‚Äôt have a personal name, but you can call me Assistant. How can I help you today?\n",
      "\n",
      "üë§ User: My favorite color is blue\n",
      "ü§ñ Agent: That's great! Blue is often associated with calmness and tranquility. Do you have a favorite shade of blue?\n",
      "\n",
      "üë§ User: What's my favorite color?\n",
      "ü§ñ Agent: You mentioned that your favorite color is blue!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Follow-up question - does it remember?\n",
    "run_conversation(\"What's my favorite color?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: What's my favorite color?\n",
      "ü§ñ Agent: I'm not sure what your favorite color is! If you tell me, I'd love to know.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "run_conversation(\"What's my favorite color?\", thread_id=\"111\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéâ Success!** The agent remembered your favorite color because:\n",
    "1. The checkpointer saved the state after the first message\n",
    "2. The same thread_id retrieved that saved state\n",
    "3. The conversation history was passed to the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Context-Dependent Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: Hello! What's your name?\n",
      "ü§ñ Agent: Hello! I don‚Äôt have a personal name, but you can call me Assistant. How can I help you today?\n",
      "\n",
      "üë§ User: My favorite color is blue\n",
      "ü§ñ Agent: That's great! Blue is often associated with calmness and tranquility. Do you have a favorite shade of blue?\n",
      "\n",
      "üë§ User: What's my favorite color?\n",
      "ü§ñ Agent: You mentioned that your favorite color is blue!\n",
      "\n",
      "üë§ User: I'm learning about RAG systems\n",
      "ü§ñ Agent: That's interesting! RAG stands for Retrieval-Augmented Generation, a technique in natural language processing that combines traditional information retrieval methods with generative models. It allows a system to retrieve relevant documents from a knowledge base and then generate responses based on that information. This can enhance the accuracy and relevance of the generated content. Do you have any specific questions about RAG systems?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Start a new topic\n",
    "run_conversation(\"I'm learning about RAG systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: Hello! What's your name?\n",
      "ü§ñ Agent: Hello! I don‚Äôt have a personal name, but you can call me Assistant. How can I help you today?\n",
      "\n",
      "üë§ User: My favorite color is blue\n",
      "ü§ñ Agent: That's great! Blue is often associated with calmness and tranquility. Do you have a favorite shade of blue?\n",
      "\n",
      "üë§ User: What's my favorite color?\n",
      "ü§ñ Agent: You mentioned that your favorite color is blue!\n",
      "\n",
      "üë§ User: I'm learning about RAG systems\n",
      "ü§ñ Agent: That's interesting! RAG stands for Retrieval-Augmented Generation, a technique in natural language processing that combines traditional information retrieval methods with generative models. It allows a system to retrieve relevant documents from a knowledge base and then generate responses based on that information. This can enhance the accuracy and relevance of the generated content. Do you have any specific questions about RAG systems?\n",
      "\n",
      "üë§ User: Can you explain the main components?\n",
      "ü§ñ Agent: Sure! The main components of a Retrieval-Augmented Generation (RAG) system typically include:\n",
      "\n",
      "1. **Retrieval Component**: \n",
      "   - This part is responsible for fetching relevant documents or pieces of information from a knowledge base or database. It often uses techniques like vector embeddings, TF-IDF, or BM25 to find the most relevant texts based on the input query.\n",
      "\n",
      "2. **Generator Component**: \n",
      "   - After retrieving relevant documents, this component generates a coherent response or text. It usually relies on a generative model, such as GPT or BART, which can take the retrieved information and create a well-formed output.\n",
      "\n",
      "3. **Integrative Mechanism**: \n",
      "   - This component integrates the retrieved documents with the generative model's context. It ensures that the generator has access to the relevant information when crafting the response.\n",
      "\n",
      "4. **Training Data**: \n",
      "   - RAG systems are typically trained on pairs of queries and relevant documents, allowing the model to learn how to effectively retrieve and generate responses.\n",
      "\n",
      "5. **Evaluation Metrics**: \n",
      "   - Systems are often evaluated using metrics that assess both the quality of the retrieval (e.g., precision, recall) and the quality of the generated output (e.g., BLEU score, ROUGE score).\n",
      "\n",
      "These components work together to create a system that can provide more accurate and contextually relevant responses than traditional generative models alone. If you have more questions about any specific component, feel free to ask!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Reference it\n",
    "run_conversation(\"Can you explain the main components?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: Hello! What's your name?\n",
      "ü§ñ Agent: Hello! I don‚Äôt have a personal name, but you can call me Assistant. How can I help you today?\n",
      "\n",
      "üë§ User: My favorite color is blue\n",
      "ü§ñ Agent: That's great! Blue is often associated with calmness and tranquility. Do you have a favorite shade of blue?\n",
      "\n",
      "üë§ User: What's my favorite color?\n",
      "ü§ñ Agent: You mentioned that your favorite color is blue!\n",
      "\n",
      "üë§ User: I'm learning about RAG systems\n",
      "ü§ñ Agent: That's interesting! RAG stands for Retrieval-Augmented Generation, a technique in natural language processing that combines traditional information retrieval methods with generative models. It allows a system to retrieve relevant documents from a knowledge base and then generate responses based on that information. This can enhance the accuracy and relevance of the generated content. Do you have any specific questions about RAG systems?\n",
      "\n",
      "üë§ User: Can you explain the main components?\n",
      "ü§ñ Agent: Sure! The main components of a Retrieval-Augmented Generation (RAG) system typically include:\n",
      "\n",
      "1. **Retrieval Component**: \n",
      "   - This part is responsible for fetching relevant documents or pieces of information from a knowledge base or database. It often uses techniques like vector embeddings, TF-IDF, or BM25 to find the most relevant texts based on the input query.\n",
      "\n",
      "2. **Generator Component**: \n",
      "   - After retrieving relevant documents, this component generates a coherent response or text. It usually relies on a generative model, such as GPT or BART, which can take the retrieved information and create a well-formed output.\n",
      "\n",
      "3. **Integrative Mechanism**: \n",
      "   - This component integrates the retrieved documents with the generative model's context. It ensures that the generator has access to the relevant information when crafting the response.\n",
      "\n",
      "4. **Training Data**: \n",
      "   - RAG systems are typically trained on pairs of queries and relevant documents, allowing the model to learn how to effectively retrieve and generate responses.\n",
      "\n",
      "5. **Evaluation Metrics**: \n",
      "   - Systems are often evaluated using metrics that assess both the quality of the retrieval (e.g., precision, recall) and the quality of the generated output (e.g., BLEU score, ROUGE score).\n",
      "\n",
      "These components work together to create a system that can provide more accurate and contextually relevant responses than traditional generative models alone. If you have more questions about any specific component, feel free to ask!\n",
      "\n",
      "üë§ User: Which component is most important?\n",
      "ü§ñ Agent: The importance of each component in a Retrieval-Augmented Generation (RAG) system can vary depending on the specific use case, but generally:\n",
      "\n",
      "1. **Retrieval Component**: \n",
      "   - This is often considered the most critical component because the quality and relevance of the retrieved information directly impact the quality of the generated response. If the retrieval fails to provide relevant documents, the generator has less context to work with, which can lead to inaccurate or irrelevant outputs.\n",
      "\n",
      "2. **Generator Component**: \n",
      "   - While the retrieval is crucial, a strong generative model is equally important. If the generator is not capable of producing coherent or contextually appropriate responses, even the best retrieved documents won't lead to effective communication.\n",
      "\n",
      "In summary, both components are essential, but effective retrieval is typically foundational. A well-designed retrieval system enhances the generator's ability to produce relevant and accurate responses.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Follow-up question\n",
    "run_conversation(\"Which component is most important?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Notice:** The agent maintains context across multiple turns - just like a real conversation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Multiple Conversations (Different Thread IDs)\n",
    "\n",
    "Let's test that different thread IDs have separate memories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîµ CONVERSATION 1\n",
      "\n",
      "üë§ User: My name is Alice\n",
      "ü§ñ Agent: Hi Alice! How can I assist you today?\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üü¢ CONVERSATION 2\n",
      "\n",
      "üë§ User: My name is Bob\n",
      "ü§ñ Agent: Nice to meet you, Bob! How can I assist you today?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Conversation 1\n",
    "print(\"\\nüîµ CONVERSATION 1\")\n",
    "run_conversation(\"My name is Alice\", thread_id=\"user_alicee\")\n",
    "\n",
    "# Conversation 2 (different user)\n",
    "print(\"\\nüü¢ CONVERSATION 2\")\n",
    "run_conversation(\"My name is Bob\", thread_id=\"user_bobb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîµ BACK TO CONVERSATION 1\n",
      "\n",
      "üë§ User: My name is Alice\n",
      "ü§ñ Agent: Hi Alice! How can I assist you today?\n",
      "\n",
      "üë§ User: What's my name?\n",
      "ü§ñ Agent: Your name is Alice! How can I help you today?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Back to Alice - does it remember her name?\n",
    "print(\"\\nüîµ BACK TO CONVERSATION 1\")\n",
    "run_conversation(\"What's my name?\", thread_id=\"user_alicee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ BACK TO CONVERSATION 2\n",
      "\n",
      "üë§ User: My name is Bob\n",
      "ü§ñ Agent: Nice to meet you, Bob! How can I assist you today?\n",
      "\n",
      "üë§ User: What's my name?\n",
      "ü§ñ Agent: Your name is Bob. How can I help you further?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Back to Bob\n",
    "print(\"\\nüü¢ BACK TO CONVERSATION 2\")\n",
    "run_conversation(\"What's my name?\", thread_id=\"user_bobb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéØ Key Insight:** Each thread_id maintains its own conversation history. This is how you'd handle multiple users in production!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Interactive Chat Loop\n",
    "\n",
    "Let's create an interactive chat session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ü§ñ Interactive Chat Started\n",
      "Type your message and press Enter. Type 'exit' to quit.\n",
      "======================================================================\n",
      "\n",
      "User: hii\n",
      "\n",
      "ü§ñ Agent: Hi there! How can I help you today?\n",
      "User: who are you\n",
      "\n",
      "ü§ñ Agent: I'm an AI assistant here to help you with information and answer your questions. What would you like to know?\n",
      "\n",
      "üëã Goodbye!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def interactive_chat():\n",
    "    \"\"\"\n",
    "    Run an interactive chat session.\n",
    "    Type 'exit' or 'quit' to stop.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ü§ñ Interactive Chat Started\")\n",
    "    print(\"Type your message and press Enter. Type 'exit' to quit.\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    thread_id = \"interactive_session2\"\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nüë§ You: \").strip()\n",
    "        \n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"\\nüëã Goodbye!\\n\")\n",
    "            break\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        # Get response\n",
    "        result = agent.invoke(\n",
    "            {\"messages\": [HumanMessage(content=user_input)]},\n",
    "            config={\"configurable\": {\"thread_id\": thread_id}}\n",
    "        )\n",
    "        \n",
    "        # Print agent's response\n",
    "        agent_message = result[\"messages\"][-1]\n",
    "        print(f\"User: {user_input}\")\n",
    "        print(f\"\\nü§ñ Agent: {agent_message.content}\")\n",
    "\n",
    "# Uncomment to run interactive chat:\n",
    "interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11: ConversationalRetrievalChain\n",
    "\n",
    "Let's compare LangGraph with the memory you learned in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationalRetrievalChain\n",
    "\n",
    "```python\n",
    "# Langchain approach\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"question\": \"What is Python?\"})\n",
    "```\n",
    "\n",
    "**Characteristics:**\n",
    "- ‚úÖ Simple to use\n",
    "- ‚úÖ Built-in memory\n",
    "- ‚ùå Fixed pipeline (always retrieves)\n",
    "- ‚ùå No conditional logic\n",
    "- ‚ùå Can't add complex decision-making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 12: LangGraph Agent\n",
    "\n",
    "```python\n",
    "# LangGraph approach\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"assistant\", assistant_node)\n",
    "memory = MemorySaver()\n",
    "agent = builder.compile(checkpointer=memory)\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is Python?\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"user_123\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "**Characteristics:**\n",
    "- ‚úÖ Flexible - add any nodes/edges\n",
    "- ‚úÖ Conditional logic (coming in Topic 2)\n",
    "- ‚úÖ Agents can make decisions\n",
    "- ‚úÖ Supports cycles and loops\n",
    "- ‚ùå More complex to set up\n",
    "- ‚ùå Requires understanding graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use What?\n",
    "\n",
    "| Use Case | LangChains | LangGraph |\n",
    "|----------|-------------------|------------------------|\n",
    "| Simple chatbot | ‚úÖ Perfect | ‚ö†Ô∏è Overkill |\n",
    "| Fixed RAG pipeline | ‚úÖ Great | ‚ö†Ô∏è Unnecessary |\n",
    "| Agent with tools | ‚ùå Limited | ‚úÖ Ideal |\n",
    "| Conditional retrieval | ‚ùå Can't do | ‚úÖ Perfect |\n",
    "| Multi-agent systems | ‚ùå Not possible | ‚úÖ Built for it |\n",
    "\n",
    "**Rule of thumb:** If you need decision-making during execution, use LangGraph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12: Summary\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **LangGraph Basics**\n",
    "   - LangGraph enables agentic behavior through graphs\n",
    "   - Better than chains when you need decisions during execution\n",
    "\n",
    "2. **Core Concepts**\n",
    "   - **State:** Data flowing through the graph (MessagesState for chat)\n",
    "   - **Nodes:** Functions that process and update state\n",
    "   - **Edges:** Connections between nodes (fixed or conditional)\n",
    "   - **Checkpointers:** Persist state for memory across sessions\n",
    "\n",
    "3. **Practical Skills**\n",
    "   - Built a stateful chatbot\n",
    "   - Used thread_id for multi-user conversations\n",
    "   - Visualized graph structure\n",
    "   - Compared with Module 9 chains\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Topic 2: Tool Integration**\n",
    "- Add tools for agents (search, calculator, retrieval)\n",
    "- Conditional edges (agent decides which tool to use)\n",
    "- This is where LangGraph really shines!\n",
    "\n",
    "**Topic 3: Agentic RAG**\n",
    "- Agent that decides when to retrieve\n",
    "- Combines everything from Topics 1-2\n",
    "- The core concept of this module!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Practice Exercises\n",
    "## Exercise 1: Build Your First Stateful Agent\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "**Estimated Time:** 30-45 minutes\n",
    "\n",
    "### Task\n",
    "Build a simple customer support chatbot that remembers conversation context.\n",
    "\n",
    "### Requirements\n",
    "1. Create a StateGraph with MessagesState\n",
    "2. Add a system prompt that makes the agent act as a helpful customer support rep\n",
    "3. Use MemorySaver checkpointer for memory\n",
    "4. Test with a multi-turn conversation where context matters\n",
    "\n",
    "### Example Conversation\n",
    "```\n",
    "User: \"I bought a laptop last week\"\n",
    "Agent: \"I'd be happy to help with your laptop! What seems to be the issue?\"\n",
    "User: \"It won't turn on\"\n",
    "Agent: \"I understand your laptop won't turn on. Have you tried...\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Reflection Questions\n",
    "\n",
    "1. **How does LangGraph's state management differ from ConversationalRetrievalChain memory?**\n",
    "   \n",
    "2. **Why do we need thread_id for conversations?**\n",
    "   \n",
    "3. **What happens if you don't configure a checkpointer?**\n",
    "   \n",
    "4. **When would you choose chains over LangGraph?**\n",
    "   \n",
    "5. **How would you debug an agent that's not behaving correctly?**\n",
    "\n",
    "Write your answers below or discuss with your study group!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**üéâ Topic 1 Complete!** \n",
    "\n",
    "You now understand LangGraph fundamentals. Next up: **Tool Integration** - where agents become truly powerful!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
